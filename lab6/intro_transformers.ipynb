{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a55892",
   "metadata": {},
   "source": [
    "# BÀI 1: KHÔI PHỤC MASKED TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526a96dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "d:\\10. ky1nam4\\NLP\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\hf_cache\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu gốc: Hanoi is the [MASK] of Vietnam.\n",
      "\n",
      "1. Dự đoán: 'capital' với độ tin cậy: 0.9991\n",
      "   -> Câu hoàn chỉnh: hanoi is the capital of vietnam.\n",
      "\n",
      "2. Dự đoán: 'center' với độ tin cậy: 0.0001\n",
      "   -> Câu hoàn chỉnh: hanoi is the center of vietnam.\n",
      "\n",
      "3. Dự đoán: 'birthplace' với độ tin cậy: 0.0001\n",
      "   -> Câu hoàn chỉnh: hanoi is the birthplace of vietnam.\n",
      "\n",
      "4. Dự đoán: 'headquarters' với độ tin cậy: 0.0001\n",
      "   -> Câu hoàn chỉnh: hanoi is the headquarters of vietnam.\n",
      "\n",
      "5. Dự đoán: 'city' với độ tin cậy: 0.0001\n",
      "   -> Câu hoàn chỉnh: hanoi is the city of vietnam.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "input_sentence = \"Hanoi is the [MASK] of Vietnam.\"\n",
    "predictions = mask_filler(input_sentence, top_k=5)\n",
    "\n",
    "print(f\"Câu gốc: {input_sentence}\\n\")\n",
    "for i, pred in enumerate(predictions, 1):\n",
    "    print(f\"{i}. Dự đoán: '{pred['token_str']}' với độ tin cậy: {pred['score']:.4f}\")\n",
    "    print(f\"   -> Câu hoàn chỉnh: {pred['sequence']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f16ca",
   "metadata": {},
   "source": [
    "1. Mô hình đã dự đoán đúng từ \"capital\" không?\n",
    "\n",
    "   Có, mô hình đã dự đoán đúng từ \"capital\" với độ tin cậy rất cao \n",
    "   (99%). Điều này cho thấy mô hình BERT đã học được mối quan hệ \n",
    "   ngữ nghĩa giữa \"Hanoi\" và \"capital of Vietnam\" rất tốt.\n",
    "\n",
    "2. Tại sao các mô hình Encoder-only như BERT lại phù hợp cho tác vụ này?\n",
    "   - BERT sử dụng cơ chế Self-Attention hai chiều (bidirectional), cho phép mô hình \n",
    "     xem xét cả các từ đứng trước và sau token [MASK].\n",
    "   - Trong câu \"Hanoi is the [MASK] of Vietnam\", BERT có thể nhìn thấy cả \"Hanoi\" \n",
    "     (trước) và \"of Vietnam\" (sau) để đưa ra dự đoán chính xác.\n",
    "   - BERT được huấn luyện trước (pre-trained) với tác vụ Masked Language Modeling (MLM), \n",
    "     nên nó rất giỏi trong việc dự đoán từ bị che giấu dựa trên ngữ cảnh xung quanh.\n",
    "   - Các mô hình Decoder-only (như GPT) chỉ nhìn một chiều (từ trái sang phải) nên \n",
    "     không thể tận dụng thông tin từ các từ phía sau token [MASK]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf215e4",
   "metadata": {},
   "source": [
    "# BÀI 2: DỰ ĐOÁN TỪ TIẾP THEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac1e6214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "d:\\10. ky1nam4\\NLP\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\hf_cache\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu mồi: 'The best thing about learning NLP is'\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Văn bản 1 được sinh ra:\n",
      "The best thing about learning NLP is its ability to teach. There is no better time to learn NLP than right now!\n",
      "\n",
      "A few years ago I wrote about how to practice using NLP as a teaching tool for aspiring programmers. In this post I'll be talking about some of my own experiences with NLP.\n",
      "\n",
      "The first time I learned NLP was when I was working on a project like this:\n",
      "\n",
      "So I was working on a very complex and very complex, very big, world, with a very large team. I tried to use NLP with great success, and in the end I got stuck.\n",
      "\n",
      "I ended up with an NLP project that I managed to pull off so easily, I had to do very little. I mean, it was so easy. I was able to make a simple app that was pretty easy to use.\n",
      "\n",
      "The next time I found myself in a very complex world, I tried to use it:\n",
      "\n",
      "But I had to make a few specific mistakes, which I did not make any mistakes on, and also because I was stuck in a very complex world, I did not have the time to learn the whole world and solve every problem.\n",
      "\n",
      "I did not learn how to use NLP in a very simple way,\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Văn bản 2 được sinh ra:\n",
      "The best thing about learning NLP is that you don't have to do everything. You take a deep breath, and you take a deep breath and you do it again and again. It's like getting a new life. You are so fresh today.\"\n",
      "\n",
      "It's the same with the rest of us.\n",
      "\n",
      "\"I can't really say that it is because I am not a better person than other people,\" he said.\n",
      "\n",
      "\"I know that I was always a better person than other people. I know that it's the most difficult thing I've ever had to do. I've never done it, I've never done it in my life.\"\n",
      "\n",
      "It's probably just the fact that NLP doesn't seem to be an option for everyone. But some of it is.\n",
      "\n",
      "The best way to learn is to learn. It's the hardest thing I've ever had to do.\n",
      "\n",
      "\"I still remember the first time I realized that I could be better than other people,\" he said, \"because I learned to read more than other people. And I learned to listen to more than other people.\"\n",
      "\n",
      "It's that hard to learn because that's the hardest thing.\n",
      "\n",
      "\"I always try to take my time when I can,\" he said.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Văn bản 3 được sinh ra:\n",
      "The best thing about learning NLP is that it allows you to study the whole world for a short period of time. You can just sit at home and watch the world around you unfold.\n",
      "\n",
      "NLP is a tool that has made me a better person. I use it for learning to focus on the things that are important to me and not worrying about what I've done. It's made my life a much better place.\n",
      "\n",
      "As a young man, I was a bit concerned with our age. I felt that if I did anything wrong, I would get the hell out of there and get a letter from the police. I thought it was the wrong thing to do.\n",
      "\n",
      "I was wrong. I said, \"I've got to do it, I've got to do it, I've got to do it, I've got to do it, I've got to do it, I've got to do it, I've got to do it, I've got to do it, I've got to do it. I've got to do it. I've got to do it, I've got to do it, I've got to do it, I've got to do it. I've got to do it, I've got to do it, I've got to\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "prompt = \"The best thing about learning NLP is\"\n",
    "generated_texts = generator(prompt, max_length=50, num_return_sequences=3)\n",
    "\n",
    "print(f\"Câu mồi: '{prompt}'\\n\")\n",
    "print(\"=\" * 70)\n",
    "for i, text in enumerate(generated_texts, 1):\n",
    "    print(f\"\\nVăn bản {i} được sinh ra:\")\n",
    "    print(text['generated_text'])\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e87bd",
   "metadata": {},
   "source": [
    "1. Kết quả sinh ra có hợp lý không?\n",
    "\n",
    "   Kết quả thường khá hợp lý về mặt ngữ pháp và có tính mạch lạc. \n",
    "   Tuy nhiên, nội dung có thể không hoàn toàn chính xác về mặt thực tế hoặc \n",
    "   có thể hơi chung chung. Mô hình GPT-2 mặc định được huấn luyện trên dữ liệu \n",
    "   tiếng Anh nên nó có khả năng sinh văn bản tiếng Anh tự nhiên và có ngữ nghĩa.\n",
    "\n",
    "2. Tại sao các mô hình Decoder-only như GPT lại phù hợp cho tác vụ này?\n",
    "    \n",
    "   - GPT được thiết kế với cơ chế Self-Attention một chiều (unidirectional/causal), \n",
    "     chỉ xem xét các token đã xuất hiện trước đó. Điều này phù hợp với tác vụ \n",
    "     sinh văn bản tuần tự (từ trái sang phải).\n",
    "   - GPT được huấn luyện với mục tiêu Next Token Prediction, tức là dự đoán token \n",
    "     tiếp theo dựa trên các token trước đó. Đây chính là bản chất của tác vụ text generation.\n",
    "   - Kiến trúc Decoder-only cho phép mô hình sinh từng token một cách tự hồi quy \n",
    "     (autoregressive), phù hợp với việc tạo ra chuỗi văn bản dài và mạch lạc.\n",
    "   - Các mô hình Encoder-only (như BERT) không phù hợp vì chúng được thiết kế để \n",
    "     hiểu ngữ cảnh (understanding), không phải sinh văn bản (generation). BERT nhìn \n",
    "     cả hai chiều nên không thể sinh văn bản tuần tự một cách tự nhiên."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20f704",
   "metadata": {},
   "source": [
    "# BÀI 3: TÍNH TOÁN VECTOR BIỂU DIỄN CỦA CÂU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84763b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tải mô hình: bert-base-uncased\n",
      "Số tham số của mô hình: 109,482,240\n",
      "Thông tin tokenization:\n",
      "Input IDs: tensor([[ 101, 2023, 2003, 1037, 7099, 6251, 1012,  102]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Tokens: ['[CLS]', 'this', 'is', 'a', 'sample', 'sentence', '.', '[SEP]']\n",
      "\n",
      "Shape của last_hidden_state: torch.Size([1, 8, 768])\n",
      "  - batch_size: 1\n",
      "  - sequence_length: 8\n",
      "  - hidden_size: 768\n",
      "\n",
      "======================================================================\n",
      "VECTOR BIỂU DIỄN CỦA CÂU:\n",
      "======================================================================\n",
      "Shape: torch.Size([1, 768])\n",
      "\n",
      "Vector (10 giá trị đầu tiên): tensor([-6.3874e-02, -4.2837e-01, -6.6779e-02, -3.8430e-01, -6.5785e-02,\n",
      "        -2.1826e-01,  4.7636e-01,  4.8659e-01,  3.9991e-05, -7.4274e-02])\n",
      "\n",
      "Norm của vector: 10.4174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Đã tải mô hình: {model_name}\")\n",
    "print(f\"Số tham số của mô hình: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "sentences = [\"This is a sample sentence.\"]\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "print(\"Thông tin tokenization:\")\n",
    "print(f\"Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"Attention Mask: {inputs['attention_mask']}\")\n",
    "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "print(f\"\\nShape của last_hidden_state: {last_hidden_state.shape}\")\n",
    "print(f\"  - batch_size: {last_hidden_state.shape[0]}\")\n",
    "print(f\"  - sequence_length: {last_hidden_state.shape[1]}\")\n",
    "print(f\"  - hidden_size: {last_hidden_state.shape[2]}\")\n",
    "\n",
    "attention_mask = inputs['attention_mask']\n",
    "mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
    "sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "sentence_embedding = sum_embeddings / sum_mask\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VECTOR BIỂU DIỄN CỦA CÂU:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Shape: {sentence_embedding.shape}\")\n",
    "print(f\"\\nVector (10 giá trị đầu tiên): {sentence_embedding[0][:10]}\")\n",
    "print(f\"\\nNorm của vector: {torch.norm(sentence_embedding).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea45e435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SO SÁNH HAI PHƯƠNG PHÁP:\n",
      "======================================================================\n",
      "1. Mean Pooling - Shape: torch.Size([1, 768])\n",
      "   Norm: 10.4174\n",
      "\n",
      "2. CLS Token - Shape: torch.Size([1, 768])\n",
      "   Norm: 14.7415\n",
      "\n",
      "Cosine Similarity giữa hai phương pháp: 0.4104\n"
     ]
    }
   ],
   "source": [
    "cls_embedding = last_hidden_state[:, 0, :]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SO SÁNH HAI PHƯƠNG PHÁP:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"1. Mean Pooling - Shape: {sentence_embedding.shape}\")\n",
    "print(f\"   Norm: {torch.norm(sentence_embedding).item():.4f}\")\n",
    "print(f\"\\n2. CLS Token - Shape: {cls_embedding.shape}\")\n",
    "print(f\"   Norm: {torch.norm(cls_embedding).item():.4f}\")\n",
    "\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "    sentence_embedding, cls_embedding, dim=1\n",
    ")\n",
    "print(f\"\\nCosine Similarity giữa hai phương pháp: {cosine_sim.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22510d78",
   "metadata": {},
   "source": [
    "1. Kích thước (chiều) của vector biểu diễn là bao nhiêu? Con số này tương ứng với\n",
    "tham số nào của mô hình BERT?\n",
    "\n",
    "Con số này tương ứng với tham số hidden_size của mô hình BERT-base. Đây là số chiều của vector embedding mà mỗi token được biểu diễn trong các hidden layers. Từ kết quả last_hidden_state.shape = [1, 8, 768], ta thấy mỗi token trong 8 tokens của câu đều được biểu diễn bằng vector 768 chiều.\n",
    "\n",
    "2. Tại sao chúng ta cần sử dụng attention_mask khi thực hiện Mean Pooling?\n",
    "\n",
    "Chúng ta cần attention_mask để loại bỏ padding tokens khỏi phép tính trung bình.\n",
    "Khi xử lý batch nhiều câu có độ dài khác nhau, tokenizer sẽ đệm (padding) các câu ngắn hơn. Padding tokens không mang ý nghĩa ngữ nghĩa, nếu tính vào trung bình sẽ làm méo mó vector biểu diễn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
